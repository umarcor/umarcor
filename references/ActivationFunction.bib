@Article{af:tarela02,
  author = {J.M. Tarela and K. Basterretxea and I. {del Campo} and M.V. Martínez and E. Alonso},
  title = {Optimised PWL recursive approximation and its application to neuro-fuzzy systems},
  journal = {Mathematical and Computer Modelling},
  volume = {35},
  number = {7},
  pages = {867-883},
  year = {2002},
  issn = {0895-7177},
  doi = {https://doi.org/10.1016/S0895-7177(02)00056-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0895717702000560},
  keywords = {PWL function, Function approximation, Neuro-fuzzy systems, Membership function circuit, Gaussian function.},
  abstract = {In this work, a piecewise-linear (PWL) function approximation is described by a lattice algebra. The maximum (∨) and minimum (∧) lattice operators have been modified to incorporate interpolation capability of generated PWL function vertexes. As a result of that, a new recursive method called centred recursive interpolation (CRI) based on such operators is proposed and analysed for successive function smoothing and more accurate approximation. The resultant computational scheme is accurate but simple, as few parameters are needed for function definition. The method is tested by applying it to the optimum approximation of some sample functions, and it turns out to be a natural quadratic approximation. Due to its advantageous characteristics and the properties that Gaussian-like function based neuro-fuzzy systems show, optimised approximation of programmable Gaussian functions has been studied in detail. A table of optimum parameters has been obtained for approximating the function through different design schemes. This constitutes a previous theoretical work for the future hardware implementation of function generators in neuro-fuzzy systems.}
}

@InProceedings{af:basterretxea04,
  author = {K. Basterretxea and J.M. Tarela and I. del Campo},
  title = {Approximation of sigmoid function and the derivative for hardware implementation of artificial neurons},
  ISSN = {1350-2409},
  booktitle = {IEE Proceedings - Circuits, Devices and Systems},
  issue = {1},
  volume = {151},
  year = {2004},
  month = {February},
  pages = {18-24(6)},
  keywords = {interpolation method;hardware implementation;learning capability;centred linear approximation;successive vertex smoothing;piecewise linear recursive approximation;error analysis;cost-effective implementation;nonlinear activation function;low memory requirements;high approximation accuracy;sigmoid function approximation;digital neural networks;artificial neurons;},
  abstract = {A piecewise linear recursive approximation scheme is applied to the computation of the sigmoid function and its derivative in artificial neurons with learning capability. The scheme provides high approximation accuracy with very low memory requirements. The recursive nature of this method allows for the control of the rate accuracy/computation-delay just by modifying one parameter with no impact on the occupied area. The error analysis shows an accuracy comparable to or better than other reported piecewise linear approximation schemes. No multiplier is needed for a digital implementation of the sigmoid generator and only one memory word is required to store the parameter that optimises the approximation.},
}

@InProceedings{af:lin08,
  author={Che-Wei Lin and Jeen-Shing Wang},
  booktitle={2008 IEEE International Symposium on Circuits and Systems (ISCAS)},
  title={A digital circuit design of hyperbolic tangent sigmoid function for neural networks},
  year={2008},
  volume={},
  number={},
  pages={856-859},
  doi={10.1109/ISCAS.2008.4541553}
}

@Article{af:armato11,
  author = {A. Armato and L. Fanucci and E.P. Scilingo and D. {De Rossi}},
  title = {Low-error digital hardware implementation of artificial neuron activation functions and their derivative},
  journal = {Microprocessors and Microsystems},
  volume = {35},
  number = {6},
  pages = {557-567},
  year = {2011},
  issn = {0141-9331},
  doi = {https://doi.org/10.1016/j.micpro.2011.05.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0141933111000731},
  keywords = {Artificial neural network, Sigmoid, Hyperbolic tangent, Piecewise linear approximation, FPGA},
  abstract = {In this paper we propose a low-error approximation of the sigmoid function and hyperbolic tangent, which are mainly used to activate the artificial neuron, based on the piecewise linear method. Here, the hyperbolic tangent is alternatively approximated by exploiting its mathematical relationship with the sigmoid function, showing better results. Special attention has been paid to study the minimum number of precision bits to achieve the convergence of a multi-layer perceptron network in finite arithmetic machine. All the approximation results show lower mean relative and absolute error than those reported in the state-of-the-art. Finally, the sigmoid digital implementation is discussed and assessed in terms of work frequency, complexity and error in comparison with the state-of-the-art.}
}

@Article{af:zamanlooy14,
  author={Zamanlooy, Babak and Mirhassani, Mitra},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  title={Efficient VLSI Implementation of Neural Networks With Hyperbolic Tangent Activation Function},
  year={2014},
  volume={22},
  number={1},
  pages={39-48},
  doi={10.1109/TVLSI.2012.2232321}
}

@InProceedings{af:thomas15,
  author = {Thomas, David B.},
  title = {A General-Purpose Method for Faithfully Rounded Floating-Point Function Approximation in FPGAs},
  year = {2015},
  isbn = {9781479986644},
  publisher = {IEEE Computer Society},
  address = {USA},
  url = {https://doi.org/10.1109/ARITH.2015.27},
  doi = {10.1109/ARITH.2015.27},
  abstract = {A barrier to wide-spread use of Field Programmable Gate Arrays (FPGAs) has been the complexity of programming, but recent advances in High-Level Synthesis (HLS) have made it possible for non-experts to easily create floating-point numerical accelerators from C-like code. However, HLS users are limited to the set of numerical primitives provided by HLS vendors and designers of floating-point IP cores, and cannot easily implement new fast or accurate numerical primitives. This paper presents a method for automatically creating high-performance pipelined floating-point function approximations, which can be integrated as IP cores into numerical accelerators, whether derived from HLS or traditional design methods. Both input and output are floating-point, but internally the function approximator uses fixed-point polynomial segments, guaranteeing a faithfully rounded output. A robust and automated non-uniform segmentation scheme is used to segment any twice-differentiable input function and produce platform-independent VHDL. The approach is demonstrated across ten functions, which are automatically generated then placed and routed in Xilinx devices. The method provides a 1.1x-3x improvement in area over composite numerical approximations, while providing similar performance and significantly better relative error.},
  booktitle = {Proceedings of the 2015 IEEE 22nd Symposium on Computer Arithmetic},
  pages = {42–49},
  numpages = {8},
  keywords = {FPGA, Floating-Point, Faithful Rounding, Function Approximation},
  series = {ARITH '15}
}

@Article{af:tsai15,
  author={Tsai, Chang-Hung and Chih, Yu-Ting and Wong, Wing Hung and Lee, Chen-Yi},
  journal={IEEE Transactions on Circuits and Systems II: Express Briefs},
  title={A Hardware-Efficient Sigmoid Function With Adjustable Precision for a Neural Network System},
  year={2015},
  volume={62},
  number={11},
  pages={1073-1077},
  doi={10.1109/TCSII.2015.2456531}
}

@InProceedings{af:abdelsalam17,
  author={Abdelsalam, Ahmed M. and Langlois, J. M. Pierre and Cheriet, F.},
  booktitle={2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  title={A Configurable FPGA Implementation of the Tanh Function Using DCT Interpolation},
  year={2017},
  volume={},
  number={},
  pages={168-171},
  doi={10.1109/FCCM.2017.12}
}

@InProceedings{af:ho17fccm,
  author={Ho, Sam M. H. and Hung, C.-H. Dominic and Ng, Ho-Cheung and Wang, Maolin and So, Hayden Kwok-Hay},
  booktitle={2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  title={A Parameterizable Activation Function Generator for FPGA-Based Neural Network Applications},
  year={2017},
  volume={},
  number={},
  pages={84-84},
  doi={10.1109/FCCM.2017.40}
}

@InProceedings{af:ho17fpt,
  author={Ho, Sam M.H. and So, Hayden Kwok-Hay},
  booktitle={2017 International Conference on Field Programmable Technology (ICFPT)},
  title={NnCore: A parameterized non-linear function generator for machine learning applications in FPGAs},
  year={2017},
  volume={},
  number={},
  pages={160-167},
  doi={10.1109/FPT.2017.8280134}
}

@article{af:kiranyaz17,
title = {Progressive Operational Perceptrons},
journal = {Neurocomputing},
volume = {224},
pages = {142-154},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.10.044},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216312851},
author = {Serkan Kiranyaz and Turker Ince and Alexandros Iosifidis and Moncef Gabbouj},
keywords = {Artificial neural networks, Multi-layer perceptrons, Progressive operational perceptrons, Diversity, Scalability},
abstract = {There are well-known limitations and drawbacks on the performance and robustness of the feed-forward, fully-connected Artificial Neural Networks (ANNs), or the so-called Multi-Layer Perceptrons (MLPs). In this study we shall address them by Generalized Operational Perceptrons (GOPs) that consist of neurons with distinct (non-)linear operators to achieve a generalized model of the biological neurons and ultimately a superior diversity. We modified the conventional back-propagation (BP) to train GOPs and furthermore, proposed Progressive Operational Perceptrons (POPs) to achieve self-organized and depth-adaptive GOPs according to the learning problem. The most crucial property of the POPs is their ability to simultaneously search for the optimal operator set and train each layer individually. The final POP is, therefore, formed layer by layer and in this paper we shall show that this ability enables POPs with minimal network depth to attack the most challenging learning problems that cannot be learned by conventional ANNs even with a deeper and significantly complex configuration. Experimental results show that POPs can scale up very well with the problem size and can have the potential to achieve a superior generalization performance on real benchmark problems with a significant gain.}
}

@INPROCEEDINGS{af:lau18,
  author={Lau, Mian Mian and Hann Lim, King},
  booktitle={2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES)},
  title={Review of Adaptive Activation Function in Deep Neural Network},
  year={2018},
  volume={},
  number={},
  pages={686-690},
  doi={10.1109/IECBES.2018.8626714}
}

@Article{af:li18,
  title = {Improving deep neural network with Multiple Parametric Exponential Linear Units},
  journal = {Neurocomputing},
  volume = {301},
  pages = {11-24},
  year = {2018},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2018.01.084},
  url = {https://www.sciencedirect.com/science/Article/pii/S0925231218301255},
  author = {Yang Li and Chunxiao Fan and Yong Li and Qiong Wu and Yue Ming},
  keywords = {Deep learning, Activation function, Weight initialization},
  abstract = {Activation function is crucial to the recent successes of deep neural networks. In this paper, we first propose a new activation function, Multiple Parametric Exponential Linear Units (MPELU), aiming to generalize and unify the rectified and exponential linear units. As the generalized form, MPELU shares the advantages of Parametric Rectified Linear Unit (PReLU) and Exponential Linear Unit (ELU), leading to better classification performance and convergence property. In addition, weight initialization is very important to train very deep networks. The existing methods laid a solid foundation for networks using rectified linear units but not for exponential linear units. This paper complements the current theory and extends it to the wider range. Specifically, we put forward a way of initialization, enabling training of very deep networks using exponential linear units. Experiments demonstrate that the proposed initialization not only helps the training process but leads to better generalization performance. Finally, utilizing the proposed activation function and initialization, we present a deep MPELU residual architecture that achieves state-of-the-art performance on the CIFAR-10/100 datasets. The code is available at https://github.com/Coldmooon/Code-for-MPELU.}
}

@Booklet{af:tomas18,
  author = {Santiago Tomás Pérez Suárez},
  title = {Design methodology of sigmoid functions for Neural Networks using lookup tables on FPGAs},
  howpublished = {EasyChair Preprint no. 583},
  doi = {10.29007/xg75},
  year = {EasyChair, 2018}
}

@InProceedings{af:nguyen18,
  author={Nguyen, Van-Tinh and Luong, Tieu-Khanh and Le Duc, Han and Hoang, Van-Phuc},
  booktitle={2018 IEEE 12th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)},
  title={An Efficient Hardware Implementation of Activation Functions Using Stochastic Computing for Deep Neural Networks},
  year={2018},
  volume={},
  number={},
  pages={233-236},
  doi={10.1109/MCSoC2018.2018.00045}
}

@Article{af:scardapane19,
  title = {Kafnets: Kernel-based non-parametric activation functions for neural networks},
  journal = {Neural Networks},
  volume = {110},
  pages = {19-32},
  year = {2019},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/j.neunet.2018.11.002},
  url = {https://www.sciencedirect.com/science/Article/pii/S0893608018303174},
  author = {Simone Scardapane and Steven {Van Vaerenbergh} and Simone Totaro and Aurelio Uncini},
  keywords = {Neural networks, Activation functions, Kernel methods},
  abstract = {Neural networks are generally built by interleaving (adaptable) linear layers with (fixed) nonlinear activation functions. To increase their flexibility, several authors have proposed methods for adapting the activation functions themselves, endowing them with varying degrees of flexibility. None of these approaches, however, have gained wide acceptance in practice, and research in this topic remains open. In this paper, we introduce a novel family of flexible activation functions that are based on an inexpensive kernel expansion at every neuron. Leveraging several properties of kernel-based models, we propose multiple variations for designing and initializing these kernel activation functions (KAFs), including a multidimensional scheme allowing to nonlinearly combine information from different paths in the network. The resulting KAFs can approximate any mapping defined over a subset of the real line, either convex or non-convex. Furthermore, they are smooth over their entire domain, linear in their parameters, and they can be regularized using any known scheme, including the use of ℓ1 penalties to enforce sparseness. To the best of our knowledge, no other known model satisfies all these properties simultaneously. In addition, we provide an overview on alternative techniques for adapting the activation functions, which is currently lacking in the literature. A large set of experiments validates our proposal.}
}

@Article{af:lopezrubio19,
  title = {Piecewise {Polynomial} {Activation} {Functions} for {Feedforward} {Neural} {Networks}},
  volume = {50},
  issn = {1573-773X},
  url = {https://doi.org/10.1007/s11063-018-09974-4},
  doi = {10.1007/s11063-018-09974-4},
  abstract = {Since the origins of artificial neural network research, many models of feedforward networks have been proposed. This paper presents an algorithm which adapts the shape of the activation function to the training data, so that it is learned along with the connection weights. The activation function is interpreted as a piecewise polynomial approximation to the distribution function of the argument of the activation function. An online learning procedure is given, and it is formally proved that it makes the training error decrease or stay the same except for extreme cases. Moreover, the model is computationally simpler than standard feedforward networks, so that it is suitable for implementation on FPGAs and microcontrollers. However, our present proposal is limited to two-layer, one-output-neuron architectures due to the lack of differentiability of the learned activation functions with respect to the node locations. Experimental results are provided, which show the performance of the proposal algorithm for classification and regression applications.},
  number = {1},
  journal = {Neural Processing Letters},
  author = {López-Rubio, Ezequiel and Ortega-Zamorano, Francisco and Domínguez, Enrique and Muñoz-Pérez, José},
  month = aug,
  year = {2019},
  pages = {121--147},
}

@Article{af:cao19,
  author={Cao, Jiuwen and Zhang, Kai and Yong, Hongwei and Lai, Xiaoping and Chen, Badong and Lin, Zhiping},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  title={Extreme Learning Machine With Affine Transformation Inputs in an Activation Function},
  year={2019},
  volume={30},
  number={7},
  pages={2093-2107},
  doi={10.1109/TNNLS.2018.2877468}
}

@InProceedings{af:baccelli20,
  author={Baccelli, Guido and Stathis, Dimitrios and Hemani, Ahmed and Martina, Maurizio},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  title={NACU: A Non-Linear Arithmetic Unit for Neural Networks},
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/DAC18072.2020.9218549}
}

@Article{af:zbigniew18,
  author = {Hajduk, Zbigniew},
  year = {2018},
  month = {11},
  pages = {563-577},
  title = {Hardware implementation of hyperbolic tangent and sigmoid activation functions},
  volume = {66},
  journal = {Bulletin of the Polish Academy of Sciences, Technical Sciences},
  doi = {10.24425/bpas.2018.124272}
}

@InProceedings{af:raut20,
  author={Raut, Gopal and Rai, Shubham and Vishvakarma, Santosh Kumar and Kumar, Akash},
  booktitle={2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)},
  title={A CORDIC Based Configurable Activation Function for ANN Applications},
  year={2020},
  volume={},
  number={},
  pages={78-83},
  doi={10.1109/ISVLSI49217.2020.00024}
}

@ARTICLE{af:tran20,
  author={Tran, Dat Thanh and Kiranyaz, Serkan and Gabbouj, Moncef and Iosifidis, Alexandros},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  title={Heterogeneous Multilayer Generalized Operational Perceptron},
  year={2020},
  volume={31},
  number={3},
  pages={710-724},
  doi={10.1109/TNNLS.2019.2914082}
}

@article{af:thanh20,
  title = {Progressive Operational Perceptrons with Memory},
  journal = {Neurocomputing},
  volume = {379},
  pages = {172-181},
  year = {2020},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2019.10.079},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231219315188},
  author = {Dat Thanh Tran and Serkan Kiranyaz and Moncef Gabbouj and Alexandros Iosifidis},
  keywords = {Generalized operational perceptron, Progressive learning, Neural architecture learning},
  abstract = {Generalized Operational Perceptron (GOP) was proposed to generalize the linear neuron model used in the traditional Multilayer Perceptron (MLP) by mimicking the synaptic connections of biological neurons showing nonlinear neurochemical behaviours. Previously, Progressive Operational Perceptron (POP) was proposed to train a multilayer network of GOPs which is formed layer-wise in a progressive manner. While achieving superior learning performance over other types of networks, POP has a high computational complexity. In this work, we propose POPfast, an improved variant of POP that signicantly reduces the computational complexity of POP, thus accelerating the training time of GOP networks. In addition, we also propose major architectural modications of POPfast that can augment the progressive learning process of POP by incorporating an information preserving, linear projection path from the input to the output layer at each progressive step. The proposed extensions can be interpreted as a mechanism that provides direct information extracted from the previously learned layers to the network, hence the term “memory”. This allows the network to learn deeper architectures and better data representations. An extensive set of experiments in human action, object, facial identity and scene recognition problems demonstrates that the proposed algorithms can train GOP networks much faster than POPs while achieving better performance compared to original POPs and other related algorithms.}
}

@article{af:wang20,
  doi = {10.1088/1674-4926/41/2/022401},
  url = {https://doi.org/10.1088/1674-4926/41/2/022401},
  year = 2020,
  month = {feb},
  publisher = {{IOP} Publishing},
  volume = {41},
  number = {2},
  pages = {022401},
  author = {Zheng Wang and Libing Zhou and Wenting Xie and Weiguang Chen and Jinyuan Su and Wenxuan Chen and Anhua Du and Shanliao Li and Minglan Liang and Yuejin Lin and Wei Zhao and Yanze Wu and Tianfu Sun and Wenqi Fang and Zhibin Yu},
  title = {Accelerating hybrid and compact neural networks targeting perception and control domains with coarse-grained dataflow reconfiguration},
  journal = {Journal of Semiconductors},
  abstract = {Driven by continuous scaling of nanoscale semiconductor technologies, the past years have witnessed the progressive advancement of machine learning techniques and applications. Recently, dedicated machine learning accelerators, especially for neural networks, have attracted the research interests of computer architects and VLSI designers. State-of-the-art accelerators increase performance by deploying a huge amount of processing elements, however still face the issue of degraded resource utilization across hybrid and non-standard algorithmic kernels. In this work, we exploit the properties of important neural network kernels for both perception and control to propose a reconfigurable dataflow processor, which adjusts the patterns of data flowing, functionalities of processing elements and on-chip storages according to network kernels. In contrast to state-of-the-art fine-grained data flowing techniques, the proposed coarse-grained dataflow reconfiguration approach enables extensive sharing of computing and storage resources. Three hybrid networks for MobileNet, deep reinforcement learning and sequence classification are constructed and analyzed with customized instruction sets and toolchain. A test chip has been designed and fabricated under UMC 65 nm CMOS technology, with the measured power consumption of 7.51 mW under 100 MHz frequency on a die size of 1.8 × 1.8 mm2.}
}

@INPROCEEDINGS{skrbek22,
  author = {Skrbek, Miroslav and Kubalík, Pavel},
  booktitle = {2022 11th Mediterranean Conference on Embedded Computing (MECO)},
  title = {Approximate arithmetic for modern neural networks and FPGAs},
  year = {2022},
  volume = {},
  number = {},
  pages = {1-4},
  doi = {10.1109/MECO55406.2022.9797141}
}
